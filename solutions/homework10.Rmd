---
title: "Homework 10"
author: "Andrei Dmitrenko"
date: "May 3, 2019"
output:
    pdf_document:
html_document:
df_print: paged
---

# Task 34

## Uniqueness of predictions from the lasso

1. Suppose that we have two solutions $\hat{\beta}^{(1)}$ and $\hat{\beta}^{(2)}$ with $X \hat{\beta}^{(1)} \neq X \hat{\beta}^{(2)}$.  
Let $c$ be the min value of the lasso criterion obtained by $\hat{\beta}^{(1)}$, $\hat{\beta}^{(2)}$. For any $0 < \alpha < 1$, we have:  
$$\frac{1}{2}  \| y - X (\alpha \cdot \beta^{(1)} + (1 - \alpha) \cdot \beta^{(2)} \|_2^2 + \lambda \| \alpha \cdot \beta^{(1)} + (1 - \alpha) \cdot \beta^{(2)} \|_1 < \alpha\cdot c\ + (1 - \alpha) \cdot c = c.$$

The strict inequality is because the function $f(u) = \|y - u\|^2_2$ is strictly convex, as well as $f(u) = \|u\|_1$. Therefore, $\alpha \cdot \beta^{(1)} + (1 - \alpha) \cdot \beta^{(2)}$ attains a lower criterion value than $c$, which gives a contradiction, $\square$.

2. By 1., any two solutions must have the same squared error loss. But the solutions also attain the same value of the lasso criterion, and if $\lambda > 0$, then they must have the same $l_1$ norm, $\square$.

# Task 35

## Ridge regression solution

$$RSS(\lambda) = y^T y - 2 \beta X^Ty + \beta^T X^T X \beta + \lambda \beta^T \beta.$$
$$\frac{d RSS(\lambda)}{d\beta}  = - 2 X^Ty + 2 X^T X \beta + 2 \lambda \beta = 0.$$
Hence,  
$$X^T X \beta + \lambda \beta = X^Ty,$$
which gives:
$$\beta = (X^T X + \lambda)^{-1} X^Ty,\ \square.$$

# Task 36

## Variable selection under various norms

### 1. Data preparation

```{r}
load("../data/yeastStorey.rda")

# split the data
X = data[,-1]
y = data[, 1]

# set random seed
set.seed(123)

# get indexes for training set
training.set.size = floor(0.7 * nrow(X))
training.set.indexes = sample(seq_len(nrow(X)), size = training.set.size)

# get training set
X.train = X[training.set.indexes,]
X.test = X[-training.set.indexes,]

# get test set
y.train = y[training.set.indexes]
y.test = y[-training.set.indexes]
```

### 2. Cross-validation

```{r}
# elastic net
library(glmnet)

results.grid = matrix(0, nrow=3, ncol=11)
rownames(results.grid) = c("alpha", "lambda", "cvm")

results.grid[1,] = seq(0, 1, 0.1)

# grid search
for (i in 1:length(results.grid[1,])){
  res = cv.glmnet(x=as.matrix(X.train), y=y.train, nfolds = 10, family = "binomial", alpha = results.grid[1,i])
  results.grid[2,i] = res$lambda.min
  results.grid[3,i] = min(res$cvm)
}

results.grid

# get best parameters
best.alpha = results.grid[1,results.grid[3,] == min(results.grid[3,])]
best.lambda = results.grid[2,results.grid[3,] == min(results.grid[3,])]

# ok, I'm getting "the best" fit
best.fit = glmnet(as.matrix(X.train), y.train, family = "binomial", alpha = best.alpha, lambda = best.lambda)
```

### 3. Predicting and plotting

```{r}
# predict with "the best" model
y.predicted = predict.glmnet(best.fit, as.matrix(X.test), type="response")

# plot error vs log lambda
plot(cv.glmnet(as.matrix(X.train), y.train, alpha = best.alpha, family = "binomial"))

# trace curve of coefficients as a function of log lambda
plot(glmnet(as.matrix(X.train), y.train, alpha = best.alpha, family = "binomial"), xvar = "lambda")

# roc
library("pROC")
roc(response=y.test, predictor = y.predicted, plot=TRUE)

# get all the coefficients
coefs = coef(best.fit)

# show chosen variables
coefs@Dimnames[[1]][as.vector(coefs[,ncol(coefs)] != 0)][-1]
```



